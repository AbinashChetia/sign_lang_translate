{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import pickle as pk\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from neural_net import NeuralNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISL Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of signs included: 30\n",
      "A: 3000\n",
      "B: 3000\n",
      "C: 3000\n",
      "D: 3000\n",
      "E: 3000\n",
      "F: 3000\n",
      "G: 3000\n",
      "H: 3000\n",
      "I: 3000\n",
      "J: 3000\n",
      "K: 3000\n",
      "L: 3000\n",
      "M: 3000\n",
      "N: 3000\n",
      "O: 3000\n",
      "P: 3000\n",
      "Q: 3000\n",
      "R: 3000\n",
      "S: 3000\n",
      "T: 3000\n",
      "U: 3000\n",
      "V: 3000\n",
      "W: 3000\n",
      "X: 3000\n",
      "Y: 3000\n",
      "Z: 3000\n",
      "del: 3000\n",
      "nothing: 3000\n",
      "space: 3000\n"
     ]
    }
   ],
   "source": [
    "isl_dir = './data/asl_data/'\n",
    "print(f'Number of signs included: {len(os.listdir(isl_dir))}')\n",
    "isl_signs = os.listdir(isl_dir)\n",
    "isl_signs.sort()\n",
    "for d in isl_signs:\n",
    "    if d == '.DS_Store':\n",
    "        continue\n",
    "    print(f'{d}: {len(os.listdir(os.path.join(isl_dir, d)))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating labelled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing image directory\n",
    "\n",
    "DATA_DIR = './data/asl_data'\n",
    "data_prefix = f'prep_data/{DATA_DIR.split(\"/\")[-1]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# initiating mediapipe configurations\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "# 1-hand vs 2-hands signs\n",
    "# signs_1_hand = ['1', '2', '3', '4', '5', '6', '7', '8', '9', 'C', 'I', 'L', 'O', 'U', 'V']\n",
    "# signs_2_hand = ['A', 'B', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'W', 'X', 'Y', 'Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [14:27<00:00, 28.92s/it]\n"
     ]
    }
   ],
   "source": [
    "# data1 = []\n",
    "# labels1 = []\n",
    "# data2 = []\n",
    "# labels2 = []\n",
    "data = []\n",
    "labels = []\n",
    "max_count = 1000 # 20\n",
    "\n",
    "for dir_ in tqdm(os.listdir(DATA_DIR)):\n",
    "    if dir_ == '.DS_Store':\n",
    "        continue\n",
    "    count = 0\n",
    "    for img_path in os.listdir(os.path.join(DATA_DIR, dir_)):\n",
    "        data_aux = []\n",
    "        img = cv2.imread(os.path.join(DATA_DIR, dir_, img_path))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        results = hands.process(img_rgb)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "                    data_aux.append(x)\n",
    "                    data_aux.append(y)\n",
    "            # if len(data_aux) == 42 and dir_ in signs_1_hand:\n",
    "            #     data1.append(data_aux)\n",
    "            #     labels1.append(dir_)\n",
    "            #     count += 1\n",
    "            # elif len(data_aux) == 84 and dir_ in signs_2_hand:\n",
    "            #     data2.append(data_aux)\n",
    "            #     labels2.append(dir_)\n",
    "            #     count += 1\n",
    "            if len(data_aux) == 42:\n",
    "                data.append(data_aux)\n",
    "                labels.append(dir_)\n",
    "                count += 1\n",
    "\n",
    "        if count == max_count:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset stored at prep_data/asl_data_2023_11_15_15_30_00.pickle\n"
     ]
    }
   ],
   "source": [
    "now = dt.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "f = open(f'{data_prefix}_{now}.pickle', 'wb')\n",
    "# pickle.dump({'data1': data1, 'labels1': labels1, 'data2': data2, 'labels2': labels2}, f)\n",
    "pickle.dump({'data': data, 'labels': labels}, f)\n",
    "print(f'Dataset stored at {data_prefix}_{now}.pickle')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model - Random Forest Classifier (Scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing model directory\n",
    "\n",
    "DATA_LOC = 'prep_data/asl_data_2023_11_15_15_30_00.pickle'\n",
    "model_prefix = f'trained_models/{DATA_LOC.split(\"/\")[-1].split(\"_\")[0]}_model'\n",
    "\n",
    "# loading labelled dataset\n",
    "\n",
    "data_dict = pickle.load(open(DATA_LOC, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing data & label variables\n",
    "\n",
    "# data1 = np.asarray(data_dict['data1'])\n",
    "# labels1 = np.asarray(data_dict['labels1'])\n",
    "# data2 = np.asarray(data_dict['data2'])\n",
    "# labels2 = np.asarray(data_dict['labels2'])\n",
    "data = np.asarray(data_dict['data'])\n",
    "labels = np.asarray(data_dict['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_pipeline(data, labels):\n",
    "    \n",
    "    # train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # training Random Forest Classifier model\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    print('Training...')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # testing the trained model\n",
    "\n",
    "    print('Testing...')\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f'{accuracy_score(y_test, y_pred) * 100}% of samples classified correctly')\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model for all signs:\n",
      "Training...\n",
      "Testing...\n",
      "93.03695768612748% of samples classified correctly\n"
     ]
    }
   ],
   "source": [
    "# print('Model for 1-hand signs:')\n",
    "# model1 = train_test_pipeline(data=data1, labels=labels1)\n",
    "# print('\\nModel for 2-hands signs:')\n",
    "# model2 = train_test_pipeline(data=data2, labels=labels2)\n",
    "print('\\nModel for all signs:')\n",
    "model = train_test_pipeline(data=data, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model stored at trained_models/asl_model_2023_11_15_15_30_35.pickle\n"
     ]
    }
   ],
   "source": [
    "# storing the model\n",
    "\n",
    "now = dt.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "f = open(f'{model_prefix}_{now}.pickle', 'wb')\n",
    "# pickle.dump({'model1': model1, 'model2': model2}, f)\n",
    "pickle.dump({'model': model}, f)\n",
    "f.close()\n",
    "print(f'Model stored at {model_prefix}_{now}.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Model - Feedforward Neural Network (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
